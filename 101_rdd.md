###RDD


1.1 安装spark集群:  
解压安装包到指定位置  
tar -zxvf spark-2.1.0-bin-hadoop2.6.tgz -C /usr/local  

1.2 配置spark：  
```bash
解压安装包到指定位置  
tar -zxvf spark-2.1.0-bin-hadoop2.6.tgz -C /usr/local  
在该配置文件中添加如下配置  
export JAVA_HOME=/usr/java/jdk1.8.0_111  
#export SPARK_MASTER_IP=node1.edu360.cn  
#export SPARK_MASTER_PORT=7077  
保存退出  
重命名并修改slaves.template文件  
mv slaves.template slaves  
vi slaves  
在该文件中添加子节点所在的位置（Worker节点）  
node2.edu360.cn  
node3.edu360.cn  
node4.edu360.cn  
保存退出
将配置好的Spark拷贝到其他节点上
scp -r spark-2.1.0-bin-hadoop2.6/ node2.edu360.cn:/usr/local/
scp -r spark-2.1.0-bin-hadoop2.6/ node3.edu360.cn:/usr/local/
scp -r spark-2.1.0-bin-hadoop2.6/ node4.edu360.cn:/usr/local/

Spark集群配置完毕，目前是1个Master，3个Work，在node1.edu360.cn上启动Spark集群
/usr/local/spark-2.1.0-bin-hadoop2.6/sbin/start-all.sh

启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：http://node1.edu360.cn:8080/
```

1.3 解决master单点问题：
```bash
Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：
Spark集群规划：node1，node2是Master；node3，node4，node5是Worker
安装配置zk集群，并启动zk集群
停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark"
1.在node1节点上修改slaves配置文件内容指定worker节点
2.在node1上执行sbin/start-all.sh脚本，然后在node2上执行sbin/start-master.sh启动第二个Master
```

1.4 执行spark程序:
```bash
/usr/local/spark-2.1.0-bin-hadoop2.6/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://node1.edu360.cn:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
/usr/local/spark-2.1.0-bin-hadoop2.6/lib/spark-examples-2.1.0-hadoop2.6.0.jar \
100
该算法是利用蒙特•卡罗算法求PI
```

1.5 启动spark shell：
```bash
/usr/local/spark-2.1.0-bin-hadoop2.6/bin/spark-shell \
--master spark://node1.edu360.cn:7077 \
--executor-memory 2g \
--total-executor-cores 2

参数：
--master spark://node1.edu360.cn:7077 指定Master的地址
--executor-memory 2g 指定每个worker可用内存为2G
--total-executor-cores 2 指定整个集群使用的cup核数为2个
```


2.1 spark与mapreduce：  
mapreduce：：基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。出于任务管道承接的，考虑，当一些查询翻译到MapReduce任务时，往往会产生多个Stage，而这些串联的Stage又依赖于底层文件系统（如HDFS）来存储每一个Stage的输出结果。spark则将中间结果存入集群中的内存，再计算，最终存入hdfs或者磁盘上。所以spark比mapreduce要快的多。  

2.2 spark的高可用集群部署：  
借助zookeeper，将master，workerd的资源信息与资源使用情况存入zookeeper，让zookeeper监督master活跃情况，当master down掉后，zookeeper启动backup备用的master。

![master](images/rdd1.png "master")

2.3 spark任务执行:    
Diver端产生运算逻辑，然后添加任务task给master，master端负责资源调度，即在哪些worker上启动进程。Master再通过RPC与worker通信，让worker启动executor以及分区数量确定。worker启动executor后，executor与driver连接，执行真正计算逻辑。

![worker](images/e1.png "worker")

3.1 RDD  

* RDD:弹性分布式数据集（Resilient Distributed Dataset）:  
是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。

* RDD属性：  

```
1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。

2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。

3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。

4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。

5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。
```

* RDD的依赖关系:

窄依赖:  
窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用


宽依赖:  
宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition


* RDD缓存:  
Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。

* DAG的生成:  
DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。
