###RDD case 1

数据如下:  
```
|http|subject|teacher|
|http://|bigdata|.edu360.cn/laozhang|
|http://|bigdata|.edu360.cn/laozhang|
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laoduan |
|http://|bigdata|.edu360.cn/laoduan |
|http://|javaee.|edu360.cn/xiaoxu   |
|http://|javaee.|edu360.cn/xiaoxu   |
|http://|javaee.|edu360.cn/laoyang  |
|http://|javaee.|edu360.cn/laoyang  |
|http://|javaee.|edu360.cn/laoyang  |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laoduan |
|http://|bigdata|.edu360.cn/laoduan |
|http://|javaee.|edu360.cn/xiaoxu  |
|http://|javaee.|edu360.cn/xiaoxu  |
|http://|javaee.|edu360.cn/laoyang  |
|http://|javaee.|edu360.cn/laoyang  |
|http://|javaee.|edu360.cn/laoyang  |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laozhao |
|http://|bigdata|.edu360.cn/laoduan |
|http://|bigdata|.edu360.cn/laoduan |
|http://|javaee| .edu360.cn/xiaoxu  |
|http://|javaee| .edu360.cn/xiaoxu   |
|http://|javaee| .edu360.cn/laoyang  |
|http://|javaee| .edu360.cn/laoyang  |
|http://|javaee| .edu360.cn/laoyang |
|http://|php|.edu360.cn/laoli  |
|http://|php|.edu360.cn/laoliu |
|http://|php|.edu360.cn/laoli |
|http://|php|.edu360.cn/laoli|
```
计算要求:找出每门学科最受欢迎的教授排名前3个的。

1.1方法一:
```scala
object Subject_teacher {
  def main(args: Array[String]): Unit = {
    //g构建spark
    val conf = new SparkConf().setAppName("Subject_teacher").setMaster("local[4]")
    val cont = new SparkContext(conf)
    //读取文件内容
    val sc = cont.textFile(args(0))
    val lines = sc.map(x => {
      val teacher = x.split("/")(3)
      val subject = x.split("/")(2).split("[.]")(0)
      //将相应字段与1组成元组
      ((subject, teacher), 1)
    })
    //通过reducebykey计算相同key的次数
    val re1 = lines.reduceByKey(_ + _)
    //利用groupby将key的第一个值即学科作为map的key聚集所有相同key的数据
    val re2 = re1.groupBy(_._1._1)
    //利用mapvalues去除map中的value，因为key值相同，所以直接排序取得排名前三的
    val r3 = re2.mapValues(_.toList.sortBy(_._2).reverse.take(3))
    println(r3.collect().toBuffer)
  }
}
```
结果如下:
```scala
((javaee,laoyang),9), ((javaee,xiaoxu),6))), (php,List(((php,laoli),3), ((php,laoliu),1))), (bigdata,List(((bigdata,laozhao),15), ((bigdata,laoduan),6), ((bigdata,laozhang),2))
```

1.2方法二:
```scala
object Subject_teacher_02 {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Subject_teacher_02").setMaster("local[4]")
    val cont = new SparkContext(conf)
    val sc = cont.textFile("D:\\w.jar\\teathear")
    //创建一个数组存放所有学科
    var arr = Array("bigdata","javaee","php")
    val try_1 = sc.map(x => {
      val teacher = x.split("/")(3)
      val subject = x.split("/")(2).split("[.]")(0)
      ((subject, teacher), 1)
    })
    val t1 = try_1.reduceByKey(_ + _)
    //利用过滤，拿到唯一学科，在此学科下直接排名即可
    for(a <- arr) {
      val filt = t1.filter(_._1._1 == a)
      val re = filt.sortBy(_._2, false).take(3)
    }

  }
}
```
1.3方法三:
```scala
object Subject_teacher_03 {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Subject_teacher_03").setMaster("local[4]")
    val cont = new SparkContext(conf)
    val sc = cont.textFile("D:\\w.jar\\teathear")
    val try_1 = sc.map(x => {
      val teacher = x.split("/")(3)
      val subject = x.split("/")(2).split("[.]")(0)
      ((subject, teacher), 1)
    })
    //获得所有学科，且为array格式
    val subjects = try_1.map(_._1._1).distinct().collect()
    //自定义一个分区器，将相同学科分为同一区
    val partpa = new SubjectP(subjects)
    val p1 = try_1.partitionBy(partpa)
    //按不同的区排名即可
    val p2 = p1.mapPartitions(it => it.toList.sortBy(_._2).take(3).iterator)
    println(p2.collect().toBuffer)


  }

  class SubjectP(sbs: Array[String]) extends Partitioner {
    //用一个hashmap存储学科与对应的区
    val hsmap = new mutable.HashMap[String, Int]()
    var c = 0
    for (sb <- sbs) {
      hsmap(sb) = c
      c += 1
    }

    override def numPartitions: Int = sbs.length

    override def getPartition(key: Any): Int = {
      val subj = key.asInstanceOf[(String, String)]._1
      hsmap(subj)
    }
  }

}
```